% Solve a Pattern Recognition Problem with a Neural Network
% Script generated by Neural Pattern Recognition app
% Created 24-Aug-2018 12:30:55
%
% This script assumes these variables are defined:
%
%   whaleInput - input data.
%   whaleTarget - target data.


% x = whaleInput;
% t = whaleTarget;
load('D:\deeplearning_project\Habituation_Nural_Networks\application1\Voiceprint_20201027\Input_file\speaker\10s\mat\mfcc\speaker_mfcc_nfft512.mat');
load('D:\deeplearning_project\Habituation_Nural_Networks\application1\Voiceprint_20201027\Input_file\speaker\10s\mat\mfcc\targets_mfcc_nfft512.mat');
result_path = 'D:\deeplearning_project\Habituation_Nural_Networks\application1\Voiceprint_20201027\Input_file\speaker\10s\mat\mfcc\result';
x = samples;
t = targets;


% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainlm';  % Scaled conjugate gradient backpropagation.
IterNum = 50;
Accuracy_Iter = zeros(IterNum,1);
recall_Iter = zeros(IterNum,1);
precision_Iter = zeros(IterNum,1);
f1_score_Iter = zeros(IterNum,1);

parfor i = 1:1:IterNum
%for i = 1:1:IterNum
 rng(i);% lm: 10 (98.66%)

% Create a Pattern Recognition Network
hiddenLayerSize = 32;
net = patternnet([hiddenLayerSize, hiddenLayerSize, hiddenLayerSize], trainFcn);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
net.input.processFcns = {'removeconstantrows','mapminmax'};
%net.output.processFcns = {'removeconstantrows','mapminmax'};

% net.input.processFcns = {'removeconstantrows','mapminmax'};
% net.output.processFcns = {'removeconstantrows','mapminmax'};
% Mapping inputs onto the read pulse amplitudes of RRAM 
% net.inputs{1}.processParams{2}.YMIN = -0.7;
% net.inputs{1}.processParams{2}.YMAX = 0.7;

% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 80/100;
net.divideParam.valRatio = 20/100;
net.divideParam.testRatio = 20/100;

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Cross-Entropy

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotconfusion', 'plotroc'};
net.plotFcns = [net.plotFcns {'plotwb'}];

% Train the Network
net.trainParam.showWindow = 0;% 关闭nntraintool的对话框
[net,tr] = train(net,x,t);
% Test the Network
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y)
tind = vec2ind(t);
yind = vec2ind(y);
% percentErrors = sum(tind ~= yind)/numel(tind);
Accuracy_Iter(i,1) = sum(tind == yind)/numel(tind);
[A,~] = confusionmat(tind, yind);
[n_class, ~] = size(A);
precision = zeros(n_class, 1);
recall = zeros(n_class, 1);
f1_score = zeros(n_class, 1);
for label =1:1:n_class
    precision(label, 1)=  A(label, label)/sum(A(:,label));
    recall(label,1) = A(label, label)/sum(A(label, :));
    f1_score(label, 1) = 2 *  precision(label, 1) *  recall(label,1)/(precision(label, 1) +  recall(label,1));
end
precision_Iter(i,1) = mean(precision);
recall_Iter(i,1) = mean(recall);
f1_score_Iter(i,1) = mean(f1_score);

% Recalculate Training, Validation and Test Performance
trainTargets = t .* tr.trainMask{1};
valTargets = t .* tr.valMask{1};
testTargets = t .* tr.testMask{1};
trainPerformance = perform(net,trainTargets,y);
valPerformance = perform(net,valTargets,y);
testPerformance = perform(net,testTargets,y);

% get weights and biases
w1 = net.IW{1}; %the input-to-hidden layer weights
w2 = net.LW{2}; %the hidden-to-output layer weights
b1 = net.b{1}; %the input-to-hidden layer bias
b2 = net.b{2}; %the hidden-to-output layer bias

% View the Network
%view(net);
end    
save(fullfile(result_path, 'accuracy_nfft512_1.mat'),'Accuracy_Iter');
save(fullfile(result_path,'precision_nfft512_1.mat'),'precision_Iter');
save(fullfile(result_path,'recall_nfft512_1.mat'),'recall_Iter');
save(fullfile(result_path,'f1_score_nfft512_1.mat'),'f1_score_Iter');
[max_accuracy,max_index] = max(Accuracy_Iter);
% epoch = tr.epoch;
% loss = tr.perf;
%save(fullfile(result_path, 'epoch_mfcc_epoch.mat'), 'epoch');
%save(fullfile(result_path, 'epoch_mfcc_loss.mat'), 'loss');
% Plots
% Uncomment these lines to enable various plots.
% figure, plotperform(tr)
% figure, plottrainstate(tr)
% figure, ploterrhist(e)
% figure, plotconfusion(t,y)
% figure, plotroc(t,y)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
% See the help for each generation function for more information.
if (false)
    % Generate MATLAB function for neural network for application
    % deployment in MATLAB scripts or with MATLAB Compiler and Builder
    % tools, or simply to examine the calculations your trained neural
    % network performs.
    genFunction(net,'myNeuralNetworkFunction');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a matrix-only MATLAB function for neural network code
    % generation with MATLAB Coder tools.
    genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a Simulink diagram for simulation or deployment with.
    % Simulink Coder tools.
    gensim(net);
end
